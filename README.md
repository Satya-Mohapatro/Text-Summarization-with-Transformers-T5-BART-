# Text-Summarization-with-Transformers-T5-BART-
End-to-end abstractive text summarization project using Transformer-based Seq2Seq models (T5/BART) fine-tuned on the CNN/DailyMail dataset. Includes preprocessing, model training, evaluation (ROUGE), inference pipeline. Fully implemented in Google Colab using Hugging Face Transformers and Datasets.
